---
title: "Does Heterogeneity Drive Social Learning?"
author: |
 | Hossein Alidaee
 | Northwestern University
format: 
  revealjs: 
    auto-animate-easing: none
    auto-animate-duration: 1
    #theme: solarized
    theme: [solarized, custom.scss]
editor: visual
toc: true 
toc-depth: 1
toc-title: Roadmap
---

# Motivation

## Importance of Technology Adoption

Technology is a significant determinant of economic growth (Klenow and Rodriguez-Clare, 1997; Hall and Jones, 1999)

<br/><br/>

::: fragment
Adoption speed has converged; Penetration hasn't (Comin and Mestieri, 2018).
:::

## Unknown Returns

Returns to new technologies are unknown (Evenson and Westphal, 1995)

<br/><br/>

::: fragment
Information on returns is important for adoption (Rogers, 1962; Jensen, 2010)
:::

<br/><br/>

::: fragment
**Where will information come from?**
:::

## Sources of Information

Common taxonomy?

<br/><br/>

**Centralized:** NGO, government, or corporation.

<br/><br/>

**Decentralized:** peers, bloggers, or learning-by-doing.

## Puzzling Behavior

Centralized sources have more data.

<br/><br/>

::: fragment
Yet, we don't trust them much more than peers. (Krishnan and Patnam, 2013; Qiao, Friedman, Tam, Zeng and Li, 2020)
:::

<br/><br/>

::: fragment
**Why?**
:::

## Possible Explanations

Mistrust is amorphous:

::: fragment
-   Exaggeration?
:::

::: fragment
-   Obfuscation?
:::

<br/><br/>

::: fragment
Homophily can't explain all:

-   Translation is possible!
:::

## People As Econometricians

Individuals face a trade-off between interval vs external validity!

::: fragment
-   Returns are heterogeneous due to diverse **contexts**
:::

::: fragment
-   Knowledge of peers' contexts minimizes bias
:::

::: fragment
-   Sample size of central sources minimizes variance
:::

::: fragment
-   Analogous to bias - variance trade-off
:::

## Preview

Today:

-   Findings from AgTech
-   Propose a (scalable) mechanism
-   Design a test
-   Implications for farmers and policymakers

<!--# Heterogeneous Returns to Technology         Returns are often heterogeneous:     Education - comparative advantage (Heckman and Li, 2004), gender (Dougherty, 2005)   Microfinance - credit constraints (Banerjee et al, 2020), business experience (Meager, 2019) Skills training - baseline profits (Lopez-Pena, 2020)   Health - age (Dupas, 2009)   Whose data do I trust? -->

# Diffusion of Agricultural Technologies

## The AgTech Divergence

```{r echo=FALSE, dev.args = list(bg = 'transparent'), fig.retina=2, strip.white=TRUE, fig.asp=0.2,fig.align='left'}

library(latex2exp)
library(tidyverse)

knitr::opts_chunk$set(
      dev = "svglite",
      fig.ext = "svg"
)
```

::: columns
::: {.column width="65%"}
![](hyv_plot2.png)
:::

::: {.column width="35%"}
::: incremental
<font size=6>

-   $2^{\text{nd}}$ divergence via Green Revolution (Huang, 2020)

<br/>

-   Structural transformation needs adoption (Suri and Udry, 2022)

<br/>

-   Information is a key barrier (Magruder, 2018) </font>
:::
:::
:::

## Traditional Extension Design

::: increment
-   Account for large fraction of government spending on agriculture (Akroyd and Smith, 2007)

-   Centralized testing of technologies

-   Centralized training of extension agents

-   Extension agents relay knowledge to farmers
:::

## What works and why?

Many papers testing extension design:

<font size="5.5px">

-   Decentralized Learning (Krishnan and Patnam, 2013; Takahashi, Mano, and Otsuka, 2019)

-   Farmer Field Days (Dar and Emerick, 2021)

-   Season-long Demonstration Plots (Maertens, Michelson, and Nourani, 2020)

-   Seed Centrality (Banerjee, Chandrasekhar, Duflo, and Jackson, 2013; Shikuku, 2019)

-   Opinion Leader Superiority (Feder and Savastano, 2006)

-   Demonstration Plot Centrality (Dar, Emerick, de Janvry, Kelley, and Sadoulet, 2020)

-   Direct Contact Farmer Training (Kondylis, Mueller, and Zhu, 2017)

-   ICT Reduction of Temporal Lag (Cole and Fernando, 2021)

</font>

## What works and why?

Does **sample size** drive learning?

<font size="5">

-   ~~Decentralized Learning (Krishnan and Patnam, 2013; Takahashi, Mano, and Otsuka, 2019)~~

-   <font style="opacity:.6"> Farmer Field Days (Dar and Emerick, 2021) </font>

-   <font style="opacity:.6"> Season-long Demonstration Plots (Maertens, Michelson, and Nourani, 2020) </font>

-   <font style="opacity:.6">Seed Centrality (Banerjee, Chandrasekhar, Duflo, and Jackson, 2013; Shikuku, 2019) </font>

-   <font style="opacity:.6">Opinion Leader Superiority (Feder and Savastano, 2006) </font>

-   <font style="opacity:.6"> Demonstration Plot Centrality (Dar, Emerick, de Janvry, Kelley, and Sadoulet, 2020) </font>

-   <font style="opacity:.6"> Direct Contact Farmer Training (Kondylis, Mueller, and Zhu, 2017) </font>

-   <font style="opacity:.6"> ICT Reduction of Temporal Lag (Cole and Fernando, 2021) </font>

</font>

## What works and why?

Does **centrality** drive learning?

<font size="5">

-   <font style="opacity:.6">Decentralized Learning (Krishnan and Patnam, 2013; Takahashi, Mano, and Otsuka, 2019) </font>

-   <font style="opacity:.6"> Farmer Field Days (Dar and Emerick, 2021) </font>

-   <font style="opacity:.6"> Season-long Demonstration Plots (Maertens, Michelson, and Nourani, 2020) </font>

-   **Seed Centrality (Banerjee, Chandrasekhar, Duflo, and Jackson, 2013; Shikuku, 2019)**

-   <font style="opacity:.6"> Opinion Leader Superiority (Feder and Savastano, 2006) </font>

-   ~~Demonstration Plot Centrality (Dar, Emerick, de Janvry, Kelley, and Sadoulet, 2020)~~

-   <font style="opacity:.6"> Direct Contact Farmer Training (Kondylis, Mueller, and Zhu, 2017) </font>

-   <font style="opacity:.6"> ICT Reduction of Temporal Lag (Cole and Fernando, 2021) </font>

</font>

## What works and why?

Does **social influence** drive learning?

<font size="5">

-   <font style="opacity:.6">Decentralized Learning (Krishnan and Patnam, 2013; Takahashi, Mano, and Otsuka, 2019) </font>

-   ~~Farmer Field Days (Dar and Emerick, 2021)~~

-   <font style="opacity:.6"> Season-long Demonstration Plots (Maertens, Michelson, and Nourani, 2020) </font>

-   **Seed Centrality (Banerjee, Chandrasekhar, Duflo, and Jackson, 2013; Shikuku, 2019)**

-   ~~Opinion Leader Superiority (Feder and Savastano, 2006)~~

-   ~~Demonstration Plot Centrality (Dar, Emerick, de Janvry, Kelley, and Sadoulet, 2020)~~

-   <font style="opacity:.6"> Direct Contact Farmer Training (Kondylis, Mueller, and Zhu, 2017) </font>

-   <font style="opacity:.6"> ICT Reduction of Temporal Lag (Cole and Fernando, 2021) </font>

</font>

## What works and why?

Does **homophily** drive learning?

<font size="5">

-   **Decentralized Learning (Krishnan and Patnam, 2013; Takahashi, Mano, and Otsuka, 2019)**

-   <font style="opacity:.6"> Farmer Field Days (Dar and Emerick, 2021) </font>

-   <font style="opacity:.6"> Season-long Demonstration Plots (Maertens, Michelson, and Nourani, 2020) </font>

-   ~~Seed Centrality (Banerjee, Chandrasekhar, Duflo, and Jackson, 2013; Shikuku, 2019)~~

-   **Opinion Leader Superiority (Feder and Savastano, 2006)**

-   <font style="opacity:.6"> Demonstration Plot Centrality (Dar, Emerick, de Janvry, Kelley, and Sadoulet, 2020) </font>

-   <font style="opacity:.6"> Direct Contact Farmer Training (Kondylis, Mueller, and Zhu, 2017) </font>

-   <font style="opacity:.6"> ICT Reduction of Temporal Lag (Cole and Fernando, 2021) </font>

</font>

## What works and why?

Does **knowledge about context** drive learning?

<font size="5">

-   **Decentralized Learning (Krishnan and Patnam, 2013; Takahashi, Mano, and Otsuka, 2019)**

-   **Farmer Field Days (Dar and Emerick, 2021)**

-   **Season-long Demonstration Plots (Maertens, Michelson, and Nourani, 2020)**

-   **Seed Centrality (Banerjee, Chandrasekhar, Duflo, and Jackson, 2013; Shikuku, 2019)**

-   **Opinion Leader Superiority (Feder and Savastano, 2006)**

-   **Demonstration Plot Centrality (Dar, Emerick, de Janvry, Kelley, and Sadoulet, 2020)**

-   **Direct Contact Farmer Training (Kondylis, Mueller, and Zhu, 2017)**

-   **ICT Reduction of Temporal Lag (Cole and Fernando, 2021)**

</font>

# Context Uncertainty As A Mechanism

## Sources of Heterogeneity

Returns are often heterogeneous:

-   **Education** - comparative advantage (Heckman and Li, 2004), gender (Dougherty, 2005)

-   **Microfinance** - credit constraints (Banerjee et al, 2020), business experience (Meager, 2019)

-   **Skills training** - baseline profits (Lopez-Pena, 2020)

-   **Health** - age (Dupas, 2009)

## Certain versus Uncertain Context

For some signals, we know the context:

-   Friend's preference for goods
-   Neighboring farmer's soil
-   Peer's access to job opportunities

::: fragment
Other times, we don't:

-   Random online reviews
-   Government plot's soil content
-   Sample for stated returns to education
:::

## Heterogeneous Risk as a Compound Lottery

::: {.fragment .fade-in-then-semi-out}
```{r echo=FALSE, dev.args = list(bg = 'transparent'), fig.retina=2, strip.white=TRUE, fig.asp=0.2,fig.align='left'}

par(mar = c(0, 0, 0, 0))
certainty_text <- TeX("$Pr(k_i) = 1$")
#certainty_text <- "hello"
par(mar = c(0, 0, 0, 0))
ggplot(data = data.frame(x1 = 0, y1 = 0), aes(x=x1, y=y1) ) + 
  geom_segment(aes( xend=x1, yend=y1+1), size=1) + 
  geom_text(label=c(certainty_text), nudge_y=1.1) + 
  theme_minimal() + 
  labs(x = TeX("$U(f(s_i))$"),
       y = TeX("$Pr(s_i | k_i)$")) + 
  xlim(-10, 10) + 
  ylim(0, 1.2) + 
  theme(axis.line=element_line(colour = 'black'),
        plot.title = element_text(size=15,face='bold'),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      #panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank(),
      axis.title.y = element_text(angle = 0, vjust=0.9)) + 
  ggtitle('Certainty') 
par(mar = c(0, 0, 0, 0))
```
:::

::: {.fragment .fade-in-then-semi-out}
```{r echo=FALSE, dev.args = list(bg = 'transparent'), fig.retina=2, strip.white=TRUE, fig.asp=0.2,fig.align='left'}

  
ggplot(data = data.frame(x1 = 0, y1 = 0), aes(x=x1, y=y1) ) + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = 0.8)) + 
  geom_text(label=c(certainty_text), nudge_y=0.7) + 
  theme_minimal() + 
  labs(x = TeX("$U(f(s_i))$"),
       y = TeX("$Pr(s_i | k_i)$")) + 
  xlim(-10, 10) + 
  ylim(0, 1.2) + 
  theme(axis.line=element_line(colour = 'black'),
        plot.title = element_text(size=15,face='bold'),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      #panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank(),
      axis.title.y = element_text(angle = 0, vjust=0.9)) + 
  ggtitle('Homogenous Risk') 
```
:::

::: {.fragment .fade-in}
```{r echo=FALSE, dev.args = list(bg = 'transparent'), fig.retina=2, strip.white=TRUE, fig.asp=0.2,fig.align='left'}

means <- c(0, 4, -3)
type_labels <- c(TeX("$Pr(k_i) = 0.33$"), 
                 TeX("$Pr(k_i) = 0.33$"),
                 TeX("$Pr(k_i) = 0.33$"))
type_label_heights <- c(0.6, 0.9, 0.4)
x_vals <- c(-3, 0, 4)
ggplot(data = data.frame(x1 = x_vals, y1 = c(0,0,0)), aes(x=x1, y=y1) ) + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[1], sd = 0.8), colour = 'red') + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[2], sd = 0.5), colour = 'blue') + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[3], sd = 1.5), colour = 'green') + 
  geom_text(aes(y=type_label_heights), label=type_labels) + 
  theme_minimal() + 
  labs(x = TeX("$U(f(s_i))$"),
       y = TeX("$Pr(s_i | k_i)$")) + 
  xlim(-10, 10) + 
  ylim(0, 1.2) + 
  theme(axis.line=element_line(colour = 'black'),
        plot.title = element_text(size=15,face='bold'),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      #panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank(),
      axis.title.y = element_text(angle = 0, vjust=0.9)) + 
  ggtitle('Heterogeneous Risk') 
```
:::

## Context and Mapping Uncertainty {auto-animate="true" auto-animate-easing="ease-in-out"}

::: {data-id="box1"}
```{r echo=FALSE, dev.args = list(bg = 'transparent'), fig.retina=2, strip.white=TRUE, fig.asp=0.2,fig.align='left'}

means <- c(0, 4, -3)
type_labels <- c(TeX("$Pr(k_i) = 0.33$"), 
                 TeX("$Pr(k_i) = 0.33$"),
                 TeX("$Pr(k_i) = 0.33$"))
type_label_heights <- c(0.6, 0.9, 0.4)
x_vals <- c(-3, 0, 4)
ggplot(data = data.frame(x1 = x_vals, y1 = c(0,0,0)), aes(x=x1, y=y1) ) + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[1], sd = 0.8), colour = 'red') + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[2], sd = 0.5), colour = 'blue') + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[3], sd = 1.5), colour = 'green') + 
  geom_text(aes(y=type_label_heights), label=type_labels) + 
  theme_minimal() + 
  labs(x = TeX("$U(f(s_i))$"),
       y = TeX("$Pr(s_i | k_i)$")) + 
  xlim(-10, 10) + 
  ylim(0, 1.2) + 
  theme(axis.line=element_line(colour = 'black'),
        plot.title = element_text(size=15,face='bold'),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      #panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank(),
      axis.title.y = element_text(angle = 0, vjust=0.9)) + 
  ggtitle('Uncertain Context') 
```
:::

::: {.fragment .fade-in}
::: {data-id="box2"}
```{r echo=FALSE, dev.args = list(bg = 'transparent'), fig.retina=2, strip.white=TRUE, fig.asp=0.2,fig.align='left'}

means <- c(-3, 0, 4)
type_labels <- c(TeX("$Pr(k_i) = 1$"), 
                 TeX("$Pr(k_i) = 0"),
                 TeX("$Pr(k_i) = 0$"))
type_label_heights <- c(0.6, 0.9, 0.4)
x_vals <- c(-3, 0, 4)
ggplot(data = data.frame(x1 = x_vals, y1 = c(0,0,0)), aes(x=x1, y=y1) ) + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[1], sd = 0.8), colour = 'red') + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[2], sd = 0.5), colour = 'gray') + 
  stat_function(fun = dnorm, n = 1001, args = list(mean = x_vals[3], sd = 1.5), colour = 'gray') + 
  geom_text(aes(y=type_label_heights), label=type_labels) + 
  theme_minimal() + 
  labs(x = TeX("$U(f(s_i))$"),
       y = TeX("$Pr(s_i | k_i)$")) + 
  xlim(-10, 10) + 
  ylim(0, 1.2) + 
  theme(axis.line=element_line(colour = 'black'),
        plot.title = element_text(size=15,face='bold'),
      axis.text.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks=element_blank(),
      legend.position="none",
      panel.background=element_blank(),
      #panel.border=element_blank(),
      panel.grid.major=element_blank(),
      panel.grid.minor=element_blank(),
      plot.background=element_blank(),
      axis.title.y = element_text(angle = 0, vjust=0.9)) + 
  ggtitle('Certain Context') 
```
:::
:::

## Context - Data Trade-off

<font size=6>
A risk averse agent is estimating $$\theta_i = \underbrace{\theta \vphantom{\gamma_i}  }_{\text{average return}} + \underbrace{\gamma_i}_{\text{context adjustment}}.$$

::: {.fragment}
They can choose among many signals $s_j = \widehat{\theta}_j = \widehat{\operatorname{E}}\left[\theta + \gamma_j\right]$.
:::

::: {.fragment}
Minimizing MSE forces a bias-variance trade-off: 

```{=tex}
\begin{aligned}
 \operatorname{E}\left[(\theta_i - \widehat{\operatorname{E}}[\theta_i | s_j, \gamma_i])^2\right]    

&= \operatorname{E}\left[\left(\theta_i - \left(\widehat{\theta}_j - \operatorname{E}[\gamma_j]  + \gamma_i\right) ]\right)^2\right] \\

&= \underbrace{\left(\operatorname{E}[\widehat{\gamma}_j] - \gamma_j\right)^2 \vphantom{\left[s\left(sdE[\gamma_j]\right)^2\right]^2}  }_{\text{Context Bias}} + \underbrace{\operatorname{E}\left[\left(\operatorname{E}[\widehat{\gamma}_j] - \widehat{\gamma}_j\right)^2\right]}_{\text{Context Variance}}  + \underbrace{\hphantom{abc}\sigma^2_j \hphantom{abc}\vphantom{\left[s\left(sdE[\gamma_j]\right)^2\right]^2}}_{\text{Est. Variance}}


\end{aligned}
```
</font>
:::

# Microfoundation

## A Stylized Model

A farmer is:

-   risk-averse
-   Bayesian
-   deciding between a traditional vs high-yield seed

::: fragment
The unknown gain from the high yield seed is heterogeneous: $$\theta_i = \underbrace{\theta \vphantom{\gamma_i}  }_{\text{average return}} + \underbrace{\gamma_i}_{\text{context adjustment}}$$
:::

## Sources of Context Heterogeneity

Why is context $\gamma_j$ heterogeneous?:

::: fragment
-   soil composition
-   labor input access
-   irrigation use
-   ...
:::

## What is known about context?

Each individual $j$ has their own $\gamma_j \sim \mathcal{N}\left(0, (\sigma_j^\gamma)^2\right).$

<br/><br/>

::: fragment
$\gamma_j$ is individual $j$'s **context**.
:::

<br/><br/>

::: fragment
$\sigma_j^\gamma$ is the **context uncertainty**.
:::

## Information Environment

<font size=6>

|                      |                   Extension Agent                   |                   Friends                    |
|:----------------|:----------------------------:|:------------------------:|
| Uninformative Prior? |                        True                         |                     True                     |
| \# of Signals        |                         $M$                         |          1 each <br> ($N$ friends)           |
| Signal Structure     | $\mathbf{s}_E = \theta_E + \boldsymbol{\epsilon}_M$ |   $s_j = \theta_j + \epsilon_j \forall j$    |
| Signal Noise         |  $\epsilon_m \sim \mathcal{N}(0, \sigma_E^2)$ iid   | $\epsilon_j \sim \mathcal{N}(0, \sigma_j^2)$ |
| Info Sharing         |          $\operatorname{E}[\mathbf{s}_E]$           |          $\mathbf{s}_j$, $\gamma_j$          |
| Common Knowledge     |        Prior, $\sigma_E$, $\sigma_E^\gamma$         |     Prior, $\sigma_j$, $\sigma_E^\gamma$     |

</font>

## Signals without Context

Imagine friend $j$ shares signal $s_j = \theta + \gamma_j + \epsilon_j$.

<br/><br/>

Without $\gamma_j$, this is a signal about $\theta_j = \theta + \gamma_j$.

## Adjusting for Context

How does sharing context $\gamma_j$ impact learning?

<br/><br/>

Consider estimating $\theta_i$ with context uncertainty:

```{=tex}
\begin{align}

\operatorname{E}[\theta_i | s_j, \sigma_j, \sigma_j^\gamma, \gamma_i] &= \operatorname{E}[s_j - \epsilon_j - \gamma_j + \gamma_i  | s_j, \sigma_j, \sigma_j^\gamma, \gamma_i] \\
&= s_j + \gamma_i - \operatorname{E}[ \epsilon_j + \gamma_j | s_j, \sigma_j, \sigma_j^\gamma, \gamma_i] \\


\end{align}
```
Knowing $\gamma_j$ allows perfect adjustment:

::: {style="margin-top:-80px;"}
```{=tex}
\begin{align}


\hphantom{\operatorname{E}[\theta_i | s_j, \sigma_j, \sigma_j^\gamma, \gamma_i]} &\hphantom{=} \hphantom{\operatorname{E}[s_j - \epsilon_j - \gamma_j + \gamma_i  | s_j, \sigma_j, \sigma_j^\gamma, \gamma_i]} \\

&= s_j + \underbrace{\left(\gamma_i - \gamma_j\right)}_{\text{Adjustment}} +  \operatorname{E}[ \epsilon_j | s_j, \sigma_j, \sigma_j^\gamma, \gamma_i] 


\end{align}
```
:::

## Risk Aversion

Because the farmer is risk averse,

$$\operatorname{E}[U(\theta_i)] \leq U(\operatorname{E}[\theta_i]).$$ Thus, all else equal, he prefers taking $\gamma_j$ out of the expectation:

```{=tex}
\begin{align}

\operatorname{E}[U(s_j + \gamma_i - \epsilon_j + \gamma_j)] &\leq U\left(\operatorname{E}[s_j + \gamma_i - \epsilon_j + \gamma_j]\right) \\

&\leq U\left(s_j + (\gamma_i -\gamma_j) - \operatorname{E}[\epsilon_j ]\right)

\end{align}
```
## Context - Data Tradeoff

But what if knowing $\gamma_j$ requires adding noise?

<br/><br/>

If $U(s_j)$ is maximized by minimizing MSE, then

```{=tex}
\begin{aligned}
U(s_j) &=  \operatorname{E}\left[(\theta_i - \widehat{\operatorname{E}}[\theta_i | s_j, \gamma_i])^2\right]    \\


&= \underbrace{\left(\operatorname{E}[\widehat{\gamma}_j] - \gamma_j\right)^2 \vphantom{\left[s\left(sdE[\gamma_j]\right)^2\right]^2}  }_{\text{Context Bias}} + \underbrace{\operatorname{E}\left[\left(\operatorname{E}[\widehat{\gamma}_j] - \widehat{\gamma}_j\right)^2\right]}_{\text{Context Variance}}  + \underbrace{\hphantom{abc}\sigma^2_j \hphantom{abc}\vphantom{\left[s\left(sdE[\gamma_j]\right)^2\right]^2}}_{\text{Est. Variance}}


\end{aligned}
```
## Picking Signals

<br/><br/>

::: {.theorem text="Maximizing Adoption"}
Adoption will be maximized by the source minimizing total uncertainty

```{=tex}
\begin{align}

\left(\frac{M}{\sigma^2_E + (\sigma_E^\gamma)^2}\right)^{-1}  

&\text{ vs } 

\left( \sum_j \frac{1}{\sigma_j^2 + (\sigma_j^\gamma)^2 }\right)^{-1}

\end{align}
```
:::

## Precision is Complementary

<br/><br/>

::: {.theorem text="Supermodular Updating"}
The value of an additional observation by the sender of the signal (i.e. observing $M + 1$ instead of $M$ farms) is increasing in context precision $\left(1/\sigma_i^\gamma\right)^2$.
:::

# Lab Experiment Design

## What do we know?

We have many field experiments!

<font size="5">

-   Decentralized Learning (Krishnan and Patnam, 2013; Takahashi, Mano, and Otsuka, 2019)

-   Farmer Field Days (Dar and Emerick, 2021)

-   Season-long Demonstration Plots (Maertens, Michelson, and Nourani, 2020)

-   Seed Centrality (Banerjee, Chandrasekhar, Duflo, and Jackson, 2013; Shikuku, 2019)

-   Opinion Leader Superiority (Feder and Savastano, 2006)

-   Demonstration Plot Centrality (Dar, Emerick, de Janvry, Kelley, and Sadoulet, 2020)

-   Direct Contact Farmer Training (Kondylis, Mueller, and Zhu, 2017)

-   ICT Reduction of Temporal Lag (Cole and Fernando, 2021)

</font>

## What's missing?

Each design has a number of confounders:

<font size=5>

-   social pressure
-   heterogeneous opportunity costs
-   correlated signals
-   correlation neglect from info sharing
-   multi-period updating
-   credit constraints
-   strategic interaction
-   informed priors
-   ...

</font>

<br/><br/>

::: fragment
None can precisely identify a mechanism.
:::

## Moving Forward

What's the path to scale?

<br/>

1.  Precisely identify the mechanism in the lab
2.  Adapt it to constraints in the field
3.  Revise

## Vignette Study Summary

-   Vignette experiment via tablet game
-   Brief survey on information sources
-   Odisha (Orissa), India
-   Smallholder farmers
-   Within-subject design

## Game Design

<font size=5> **Background**: Technology, Village

**Outcome**: Adoption intensity (0-10)

::: r-stack
::: {.fragment .fade-in-then-out}
**Data**: Each NPC has two features:

-   Plot type (blue vs orange)
-   Satisfaction (1-7)
-   Plot visibility (colored vs gray)
:::

::: {.fragment .fade-in}
![](complex_map_1.png){fig-align="center" width="800"}
:::
:::

**Your Info**:

-   You plot is orange type.
-   Blue to orange conversion

</font>

## Histogram View

Users have the choice of two views:

::: panel-tabset
### Map View

![](complex_map_1.png){fig-align="center" width="500"}

### Histogram View

![](complex_histogram_1.png){fig-align="center" width="700"}
:::

## Experimental Variation

::: columns
::: {.column width="35%"}
<font size=5>

**Within Variation**:

-   \% of Gray Tiles

<br/>

**Across Variation**:

-   Round Order
-   Village, Tech

<br/>

**Constant**:

-   Blue/Orange Ratio
-   Signal distribution

</font>
:::

::: {.column width="65%"}
![](complex_map_1.png){fig-align="center" width="600"}

![](complex_map_2.png){fig-align="center" width="600"}
:::
:::

## Estimation

```{=tex}
\begin{align}

\text{Adoption Level}_{ij}=\beta_H \mathbb{1}(\text{High Type Uncertainty}) + \\
\beta_G \text{Game Controls} + \beta_D \text{Demographic Controls}

\end{align}
```
::: columns
::: {.column width="50%"}
**Game Controls**:

-   Round Number
-   Technology
-   Village
:::

::: {.column width="50%"}
**Demographic Controls**:

-   Age
-   Household Size
-   Income
-   Education
-   Game Behavior
:::
:::

## Testing Complementarity

::: {.theorem text="Supermodular Updating"}
The value of an additional observation by the sender of the signal (i.e. observing $M + 1$ instead of $M$ farms) is increasing in context precision $\left(1/\sigma_i^\gamma\right)^2$.
:::

![](DiffInDiffTable.png){fig-align="center" width="600"}

## Control Modules

**Additional modules for**:

-   Practice rounds
-   Ability to translate across types
-   Bounding heuristic behavior

## Structural Estimation

Using the diff-in-diff design, we can structurally estimate risk aversion.

<br/>

Assume CARA utility: $u(\theta_i) = 1 - e^{-a\theta_i}$

<br/>

Klibanoff et al. (2005) allows testing $$a_C \lesseqqgtr a_S$$

# Field Experiment Design

## Disaggregating Mincerian Earnings

You're deciding whether to enroll your daughter in school for an extra year. I expose you to one of three main treatment arms:

-   I show you returns to this year of education for a random person
-   I show you returns for individuals similar to your daughter in terms of grades
-   I show you returns for individuals with worse grades

# Conclusion

## Potential Implications

Potential lessons:

-   Info campaigns should provide specific returns
-   Information campaigns should disaggregate returns
-   Distributed, local experimentation could increase trust
-   Insurance with low basis-risk, when tied to experimentation, can have high positive externalities
-   Reinforces Oates (1972, 1999) argument in favor of decentralization

## Future area of study

-   Barriers to signal translation
-   Specific vs disaggregated signals

# Thank You
